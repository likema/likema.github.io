<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ubuntu | Like的世界]]></title>
  <link href="http://www.malike.net.cn/blog/categories/ubuntu/atom.xml" rel="self"/>
  <link href="http://www.malike.net.cn/"/>
  <updated>2016-04-30T09:14:36+00:00</updated>
  <id>http://www.malike.net.cn/</id>
  <author>
    <name><![CDATA[Like Ma]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SSH翻墙集群]]></title>
    <link href="http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster/"/>
    <updated>2015-03-15T13:30:43+00:00</updated>
    <id>http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster</id>
    <content type="html"><![CDATA[<p>SSH动态代理是国内较为常见的翻墙方法。正如<a href="/blog/2014/10/27/ssh-tunnel-tutorial/">SSH隧道简介</a>所说，它存在不少有优点。</p>

<p>然而在实际使用中，它存在如下缺点：</p>

<ul>
<li>与PPTP等VPN协议相比，它的连接不稳定。前者应该具备协议级断线重传机制。</li>
<li>基于廉价VPS，导致它的连接不稳定。而且廉价VPS容易掉线，有时需要用户自己找在线客户修复，进一步延长了掉线时间。</li>
<li>由于上述缺点，不适合小型公司多人使用。</li>
</ul>


<p>在大概2年前，我摸索出SSH动态代理集群的办法。并将之部署于我所服务的公司，成功负载了20-30人日常翻墙学习与工作的需求。</p>

<h2>原理</h2>

<p>SSH动态代理，即为SOCK5代理，所以我们需要的是SOCK5集群。</p>

<p>若搜索<a href="https://www.google.com/search?hl=en&amp;q=socks+5+load+balance">socks 5 load balance</a>不难发现一些有用信息：</p>

<p><a href="http://serverfault.com/questions/517971/what-is-the-best-way-to-load-balance-multiple-sock5-proxys-on-seperate-vms-in-t">What is the best way to load balance multiple sock5 proxys on seperate VM&rsquo;s in the same datacenter?</a></p>

<p>我将分别介绍3种方法搭建SOCK5集群：</p>

<ol>
<li>利用第三方模块<a href="https://github.com/yaoweibin/nginx_tcp_proxy_module">nginx_tcp_proxy_module</a>。</li>
<li>Nginx 1.9开始支持<a href="http://nginx.com/resources/admin-guide/tcp-load-balancing/">TCP Load Balancing</a>。</li>
<li><a href="http://www.haproxy.org/">HAProxy</a></li>
</ol>


<p>关于SSH动态代理的配置方法，请参看<a href="/blog/2014/12/23/autossh-tutorial/">AutoSSH简介</a></p>

<h2>nginx_tcp_proxy_module的配置方法</h2>

<p>Ubuntu的Nginx并没有将nginx_tcp_proxy_module编译进去。为了简化安装，我基于Ubuntu的Nginx包，做了Nginx的<a href="https://launchpad.net/~likemartinma/+archive/ubuntu/net">PPA</a>:</p>

<ul>
<li>升级Nginx版本</li>
<li>加入nginx_tcp_proxy_module</li>
</ul>


<p>添加我的PPA</p>

<pre><code class="bash">sudo add-apt-repository ppa:likemartinma/net
sudo apt-get -y update
</code></pre>

<p>若未安装nginx，则</p>

<pre><code class="bash">sudo apt-get install -y nginx
</code></pre>

<p>若已安装nginx，则</p>

<pre><code class="bash">sudo apt-get -y upgrade
</code></pre>

<p>在/etc/nginx/nginx.conf中，增加如下内容：</p>

<pre><code class="nginx">tcp {
    access_log /var/log/nginx/tcp_access.log;

    upstream ssh_cluster {
        # simple round-robin
        server 127.0.0.1:12345;
        server 127.0.0.1:12346;
        server 127.0.0.1:12347;

        check interval=3000 rise=2 fall=5 timeout=1000;
    }

    server {
        listen 9999;
        proxy_pass ssh_cluster;
    }
}
</code></pre>

<p>为了查看集群的状态，在/etc/nginx/sites-enabled/default的中，增加如下内容：</p>

<pre><code class="nginx">server {
    ...

    location /status {
        tcp_check_status;
    }
}
</code></pre>

<p>重启Nginx:</p>

<pre><code>service nginx restart
</code></pre>

<p>如此，访问http://&lt;cluster IP&gt;/status将能查看集群的详细状态。</p>

<h2>Nginx 1.9的配置方法</h2>

<p>Ubuntu 15.10之前的官方Nginx版本都小于1.9，须通过ppa:nginx/development升级nginx。</p>

<p>添加ppa:nginx/development</p>

<pre><code class="bash">sudo add-apt-repository ppa:nginx/development
sudo apt-get -y update
</code></pre>

<p>若未安装nginx，则</p>

<pre><code class="bash">sudo apt-get install -y nginx
</code></pre>

<p>若已安装nginx，则</p>

<pre><code class="bash">sudo apt-get -y upgrade
</code></pre>

<p>在/etc/nginx/nginx.conf中，增加如下内容：</p>

<pre><code class="nginx">stream {
    upstream ssh_cluster {
        least_conn;
        server 127.0.0.1:12345;
        server 127.0.0.1:12346;
        server 127.0.0.1:12347;
    }

    server {
        listen 9999;
        proxy_pass ssh_cluster;
    }
}
</code></pre>

<p>重启Nginx:</p>

<pre><code>service nginx restart
</code></pre>

<h2>HAProxy的配置方法</h2>

<p>安装haproxy</p>

<pre><code>sudo apt-get install -y haproxy
</code></pre>

<p>在/etc/haproxy/haproxy.cfg中，增加如下内容：</p>

<pre><code>frontend socks5
    mode tcp
    bind *:9999
    default_backend ssh_cluster

backend ssh_cluster
    mode tcp
    balance roundrobin
    server vps1 127.0.0.1:12345 weight 1 check inter 30000
    server vps2 127.0.0.1:12346 weight 1 check inter 30000
    server vps3 127.0.0.1:12347 weight 1 check inter 30000
</code></pre>

<p>为了查看集群的状态，在/etc/haproxy/haproxy.cfg中，增加如下内容：</p>

<pre><code>listen stats :9090
    balance
    mode http
    stats enable
    stats auth admin:admin
</code></pre>

<p>默认安装，haproxy处于不活动状态，须要激活它。</p>

<p>在/etc/default/haproxy中，修改如下行：</p>

<pre><code>ENABLED=1
</code></pre>

<p>最后，启动haproxy:</p>

<pre><code>service haproxy start
</code></pre>

<p>如此，访问http://&lt;cluster IP&gt;:9090/haproxy?stats将能查看集群的详细状态。</p>

<h2>总结</h2>

<ul>
<li>nginx_tcp_proxy_module有简单的集群状态页面。</li>
<li>nginx 1.9没有集群状态查页面，仅能通过错误日志/var/log/ngnix/error.log来查看掉线的集群节点。</li>
<li>haproxy不仅有完善的集群状态页面，而且不需要任何PPA，应该是最佳选择。</li>
<li>上述3种方法都缺乏认证机制，只能部署于家庭或企业内网。当然也可以部署于个人电脑，事实上，我就是这样使用的。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AutoSSH简介]]></title>
    <link href="http://www.malike.net.cn/blog/2014/12/23/autossh-tutorial/"/>
    <updated>2014-12-23T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/12/23/autossh-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://www.harding.motd.ca/autossh/">autossh</a> (Automatically restart SSH sessions and tunnels)，它在运行的时候启动一个SSH进程，并监控该进程的健康状况；当SSH进程崩溃或停止通信时，它将重启动SSH进程。</p>

<h2>命令选项</h2>

<pre><code class="bash">autossh [-V] [-M port[:echo_port]] [-f] [SSH_OPTIONS]
</code></pre>

<ul>
<li><p><strong>-M port[:echo_port]</strong> 指定监控端口（和echo端口，默认为前者加1）。</p>

<ul>
<li>若希望使远程标准inetd的echo服务（默认端口为7），则指定echo_port，仅需服务监听地址为localhost。</li>
<li>若port设置为0，则将禁用监控功能。仅在ssh退出后重启它。</li>
</ul>
</li>
<li><p><strong>-f</strong> 使autossh在后台运行。</p></li>
</ul>


<p>另外，autossh还提供了一组环境变量来控制其行为, 这里仅介绍几个有代表性的，其可以man autossh</p>

<ul>
<li><strong>AUTOSSH_FIRST_POLL</strong> 指定首次论询测试时间。</li>
<li><strong>AUTOSSH_POLL</strong> 指定连接论询时间，默认600。若该值小于两次网络超时（默认15秒），则网络超将被调整为该值的1/2</li>
<li><strong>AUTOSSH_GATETIME</strong> 指定等待SSH连接成功建立的时间，默认30秒，超时表示首次运行失败，将退出autossh。若设为0，则禁用该功能，通常用于启动时运行autossh。</li>
<li><strong>AUTOSSH_MAXLIFETIME</strong> 指autossh最长运行时间，达到该时间，autossh将退出，并杀死SSH进程。</li>
<li><strong>AUTOSSH_MAXSTART</strong> 指定SSH最大启动次数。默认-1，表示无限制。</li>
</ul>


<h2>Ubuntu配置方法</h2>

<p>基于Ubuntu 12.04或14.04，以SSH动态代理（即SSH翻墙）为例。init script和Upstart都可以将autossh变成服务，然Upstart的<em>respawn</em>容错能力更强，它能在服务进程掉线，重新启动该服务。</p>

<pre><code># autossh

description "autossh daemon"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid like
setgid like

exec /usr/bin/autossh -M64000 -q -N -D localhost:12348 sshproxy
</code></pre>

<ul>
<li><em>setuid</em>和<em>setgid</em>为了让autossh运行在指定的用户和用户组上。</li>
<li><em>start on</em>表示当eth0或wlan0激活时，启动autossh，<em>stop on</em>反之。其目的为避免系统启动或网络掉线时，频繁尝试启动autossh。</li>
</ul>


<h2>更好的办法</h2>

<p>最近OpenSSH都支持选项<strong>ServerAliveInterval</strong>和<strong>ServerAliveCountMax</strong>，实际为建立在SSH协议上的心跳测试。当测试失败后，SSH客户端进程将退出。通过Upstart的respawn功能重启SSH客户端进程，也能达到autossh目的。</p>

<p>仍以SSH动态代理为例：</p>

<pre><code># sshproxy

description "ssh proxy"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid like
setgid like

exec /usr/bin/ssh \
    -oServerAliveInterval=300 \
    -oServerAliveCountMax=2 \
    -q -N -D localhost:12348 sshproxy
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rinse简介——Debian/Ubuntu中创建RPM安装环境]]></title>
    <link href="http://www.malike.net.cn/blog/2014/07/15/rinse-tutorial/"/>
    <updated>2014-07-15T12:34:58+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/07/15/rinse-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://collab-maint.alioth.debian.org/rinse/">Rinse</a>是一个Debian环境中创建RPM发行版本（如CentOS，Scientific Linux和openSUSE）的工具。你可以利用它轻松创建各种RPM发行版本的chroot环境。</p>

<p>以下基于Ubuntu 12.04 amd64，主要以创建CentOS 6 x86_64为例。</p>

<h2>安装Rinse</h2>

<pre><code class="sh">sudo apt-get install -y rinse
</code></pre>

<h2>创建CentOS 6</h2>

<pre><code class="sh">sudo rinse --distribution centos-6 --arch amd64 --directory centos-6
</code></pre>

<p>运行该命令将创建CentOS 6 amd64于当前工作目录的centos-6目录中。其中，</p>

<ul>
<li>&ndash;distribution指定发行版本，类似还可以centos-{4,5}， fedora-core-{4,5,6,7,8,9}和opensuse-{10.1,10.2,10.3,11.0,11.1,12.1}等。可以下述命令获取：</li>
</ul>


<pre><code class="sh">rinse --list-distributions
</code></pre>

<p>具体对应于/etc/rinse/*.packages的模板名，它们主要包含RPM包列表。换一句话说，你根据需要定制自己的模板。另一方面，你也可以通过&ndash;pkgs-dir指定不同于/etc/rinse的模板目录。</p>

<ul>
<li>&ndash;arch指定架构，amd64表示64位架构，i386表示32位架构。缺省为i386.</li>
<li>&ndash;directory指定为安装目录，安装结束后便可以chroot该目录了。</li>
</ul>


<p>另外，需要额外安装某些包，可以通过指定&ndash;add-pkg-list来完成。</p>

<h2>配置RPM缓存</h2>

<p>rinse默认使用/var/rinse/cache作为缓存目录，它大大缩短了重复运行同样命令的时间。具体通过：</p>

<ul>
<li>&ndash;cache 0指禁用缓存，缺省为1</li>
<li>&ndash;cache-dir指定不同于/var/rinse/cache作为缓存目录。</li>
<li>&ndash;clean-cache指清楚缓存</li>
</ul>


<h2>定制安装后执行脚本</h2>

<p>&ndash;after-post-install, &ndash;before-post-install和&ndash;post-install顾名思义，需要指出的是&ndash;post-install默认执行/usr/lib/rinse/<distribution>/post-install.sh.</p>

<h2>如何提高安装速度？</h2>

<p>通过修改/etc/rinse/rinse.conf中对应发行版的镜像地址可以加速安装，如CentOS 6 x86_64的镜像地址可以修改为</p>

<pre><code>http://centos.ustc.edu.cn/centos/6/os/x86_64/CentOS/
</code></pre>

<p>也可以通过&ndash;config指定不同于/etc/rinse/rinse.conf的配置文件。</p>

<p>若内网存在HTTP cache服务器（如Squid)，还可以设置环境变量http_proxy来缓存rpm以及加速安装，如：</p>

<pre><code class="sh">sudo http_proxy=http://&lt;http proxy address&gt; rinse --distribution centos-6 --arch amd64 --directory centos-6
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LXC简介]]></title>
    <link href="http://www.malike.net.cn/blog/2013/11/10/lxc-tutorial/"/>
    <updated>2013-11-10T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2013/11/10/lxc-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://linuxcontainers.org/">LXC</a> (Linux Containters) 是一种基于内核容器属性的用户空间接口。 它被认为介于chroot和完全虚拟化之间，其目标为创建一个不需要独立内核，但近可能接近标准Linux安装的环境。</p>

<p>其特性如下：</p>

<ul>
<li>内核空间（ipc, uts, mount, pid, network和user)</li>
<li>支持Apparmor和SELinux</li>
<li><a href="http://en.wikipedia.org/wiki/Seccomp">seccomp</a>策略</li>
<li>chroots (使用pivot_root)</li>
<li><a href="https://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.2/capfaq-0.2.txt">Kernel capabilities</a></li>
<li>支持<a href="http://en.wikipedia.org/wiki/Cgroups">cgroups</a> (Control groups)</li>
</ul>


<p>由于没有完全虚拟化CPU，也没有虚拟化硬盘，其性能是与物理机接近的。实际经验，我发现在LXC中编译C/C++源码（如构建基于C/C++源码的RPM或DEB）的性能是VirtualBox的3倍。强烈建议使用LXC代替KVM和VirtualBox作为RPM或DEB的构建环境。</p>

<p>以下操作基于Ubuntu 12.04，那么需要安装LXC包：</p>

<pre><code class="sh">sudo apt-get install -y lxc
</code></pre>

<h2>创建LXC</h2>

<p>以创建一个名为precise的Ubuntu 12.04容器为例。</p>

<p>需要创建一个基础的配置文件。由于创建LXC完成后，不再需要该配置文件（可以删除），故该文件的名字和路径没有特殊要求。这里命名为precise.conf，放在当前路径下：</p>

<pre><code>lxc.network.type = veth
lxc.network.flags = up
lxc.network.name = eth0
lxc.network.link = lxcbr0
</code></pre>

<p>lxcbr0为由LXC包创建的虚拟网桥，通过ifconfig可以知道其IP地址10.0.3.1，网段10.0.3.1/24，容器将通过lxcbr0与外界通信。</p>

<p>如此，可以开始创建容器了：</p>

<pre><code class="sh">sudo lxc-create -n precise -f precise.conf -t ubuntu -- -r precise
</code></pre>

<ul>
<li>-n指定容器名，这里为precise。</li>
<li>-f指定基础配置文件，即上一步骤创建的precise.conf。</li>
<li>-t指定模板名，这里必须为ubuntu（创建Ubuntu 12.04)。每个模板名，对应一个脚本，它们存放在/usr/lib/lxc/templates目录（文件名形如lxc-&lt;模板名>）中。</li>
<li>&ndash;以后的参数被传递给模板脚本；</li>
<li>-r为ubuntu模板脚本的参数，表示<a href="http://en.wikipedia.org/w/index.php?title=Ubuntu_(operating_system)#Releases">Ubuntu发行版代号</a>，这里必须为precise（它是12.04的发行代号）。</li>
</ul>


<p>创建过程可能会比较漫长。通过阅读/usr/lib/lxc/templates/lxc-ubuntu，不难发现创建ubuntu容器主要依靠deboostrap来完成。另一方面，变量MIRROR和SECURITY_MIRROR决定了镜像的设置，它们默认为：</p>

<pre><code>MIRROR=http://archive.ubuntu.com/ubuntu
SECURITY_MIRROR=http://security.ubuntu.com/ubuntu
</code></pre>

<p>在大陆地区，使用默认镜像的网速较慢。为了加快创建过程，可以将它们都换成大陆或香港的镜像，如<a href="http://ftp.cuhk.edu.hk/pub/Linux/ubuntu">http://ftp.cuhk.edu.hk/pub/Linux/ubuntu</a></p>

<p>具体问题是lxc-ubuntu并没有提供命令行参数来设置MIRROR和SECURITY_MIRROR。在不修改lxc-ubuntu代码的情况下，唯一的办法就是通过设置相关环境变量来达到这个目的，如：</p>

<pre><code class="sh">sudo MIRROR="http://ftp.cuhk.edu.hk/pub/Linux/ubuntu" \
     SECURITY_MIRROR="http://ftp.cuhk.edu.hk/pub/Linux/ubuntu" \
     lxc-create -n precise -f precise.conf -t ubuntu -- -r precise
</code></pre>

<p>我司为了加快内部开发和测试人员安装Ubuntu，部署了<a href="https://www.unix-ag.uni-kl.de/~bloch/acng/">apt-cacher-ng</a>——一种deb包HTTP缓存代理。由于lxc-ubuntu基于deboostrap，可以通过设置环境变量http_proxy来设置deboostrap的缓存代理（请见<a href="http://unix.stackexchange.com/questions/38993/global-cache-config-of-debootstrap">Global cache config of debootstrap</a>）:</p>

<pre><code class="sh">sudo http_proxy="http://192.168.88.10:3142/" \
     MIRROR="http://ftp.cuhk.edu.hk/pub/Linux/ubuntu" \
     SECURITY_MIRROR="http://ftp.cuhk.edu.hk/pub/Linux/ubuntu" \
     lxc-create -n precise -f precise.conf -t ubuntu -- -r precise
</code></pre>

<p>如此，在尽可能节约外部带宽的同时，最大限度的加快了创建过程。</p>

<p>上述创建方法，容器的架构将与host os的相同（如amd64）。若需要在amd64的host os上创建i386或i686架构的容器，则需要通过模板脚本的-a参数指定i686，如：</p>

<pre><code class="sh">sudo lxc-create -n precise -f precise.conf -t ubuntu -- -r precise -a i686
</code></pre>

<h2>启动LXC</h2>

<p>若需立即启动LXC，则：</p>

<pre><code class="sh">sudo lxc-start -n precise
</code></pre>

<p>若需以daemon方式运行，则:</p>

<pre><code class="sh">sudo lxc-start -n precise -d
</code></pre>

<p>若需随host os启动而自动启动，则:</p>

<pre><code class="sh">sudo ln -s /var/lib/lxc/precise/config /etc/lxc/auto/precise.conf
</code></pre>

<h2>打开LXC控制台</h2>

<p>在没有给容器设置IP时，打开其控制台</p>

<pre><code class="sh">sudo lxc-console -n precise
</code></pre>

<p>将看到文本登录界面。 通过按热键ctrl-a和q，可以退出容器控制台。</p>

<p>更多的时候，通过ssh登录将更方便，特别是key认证方式登录。</p>

<h2>停止LXC</h2>

<p>多数情况下，可以通过在guest os（容器）内执行poweroff或shutdown -h now来关闭容器。但有些时候却需要在host os上强行关闭容器，如：</p>

<pre><code class="sh">sudo lxc-stop -n precise
</code></pre>

<h2>删除LXC</h2>

<p>容器创建后，配置和数据存放在/var/lib/lxc/precise目录中。执行</p>

<pre><code class="sh">sudo lxc-destroy -n precise
</code></pre>

<p>与手动删除该目录效果一样。</p>

<h2>其他模板</h2>

<ul>
<li>ubuntu-cloud：从Ubuntu云上下载根文件系统镜像。</li>
<li>fedora: 它依赖于yum包，通过模板脚本参数-R指定版本号，如19和20都无法创建成功。默认版本号为14，可以继续安装。</li>
<li>opensuse：它依赖于zypper，Ubuntu 12.04默认没有zypper包。虽然<a href="https://launchpad.net/~thopiekar/+archive/zypper">ppa:thopiekar/zypper</a>提供了zypper，但是创建失败。</li>
<li>busybox：仅有busybox的容器，默认不能远程登录，可以用于练习简单的命令行操作。</li>
<li>sshd：将host os中各个系统目录（/bin, /sbin/和/lib等）以只读方式绑定到容器中，仅运行ssh服务器，支持ssh登录，可用于练习复杂的命令行操作。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu 11.10 VirtualBox的Host-only网卡上外网和DHCP永久地址]]></title>
    <link href="http://www.malike.net.cn/blog/2011/12/20/vbox-hostonly-dhcp/"/>
    <updated>2011-12-20T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2011/12/20/vbox-hostonly-dhcp</id>
    <content type="html"><![CDATA[<p>VirtualBox支持各种虚拟网络：NAT, Bridge Adapter, Internal Network和Host-only Adapter等。其中Bridged Adapter最为简单和常用，它几乎是0配置，直接桥接有线或无线物理网卡就可以与互联网通信。</p>

<p>然而，我工作场所内部网和家里内部网的网段不相同，DHCP存在一定租赁时间，如果使用Bridged Adapter并DHCP获取IP地址的时候，虚拟机地址经常会改变。为此，我将笔记本电脑的VirtualBox虚拟机都修改为Host-only Adapter模式。</p>

<p>一个问题是Host-only Adapter（网段为192.168.56.0/24）默认不能与互联网通信。google之后发现网上早有人遇到类似问题，他们给出的解决办法是在/etc/rc.local中加入：</p>

<pre><code class="sh">iptables -t nat -I POSTROUTING -s 192.168.56.0/24 -j MASQUERADE
</code></pre>

<p>另一个问题是VirtualBox内置DHCP的IP租赁时间设置，也无法将MAC地址与IP地址静态绑定，这造成虚拟机IP地址每隔一段时间改变一次，给使用带来诸多不方便。另一方面，我也不想静态设置IP地址，因为如果这样做，我必须每安装一次虚拟机都要重新设置IP地址。</p>

<p>以前就听说过dnsmasq，不仅集成DNS、DHCP和TFTP功能，而且占用资源很少，设置也相对简单。</p>

<ul>
<li>安装dnsmasq</li>
</ul>


<pre><code class="sh">sudo apt-get install dnsmasq
</code></pre>

<ul>
<li>打开/etc/dnsmasq.conf，针对vboxnet0配置DHCP。</li>
</ul>


<pre><code class="plain">interface=vboxnet0

# 192.168.56.1是默认网关（host机器的vboxnet0地址）
# 208.67.222.222和208.67.220.220是DNS地址(这里使用了OpenDNS)
dhcp-option=vboxnet0,option:dns-server,192.168.56.1,208.67.222.222,208.67.220.220

# 192.168.56.2和192.168.56.254为分配地址范围
# infinite表示IP永远不过期
dhcp-range=vboxnet0,192.168.56.2,192.168.56.254,infinite
</code></pre>

<ul>
<li>重启动dnsmasq</li>
</ul>


<pre><code class="sh">sudo service dnsmasq restart
</code></pre>

<p>当然，dnsmasq也支持MAC地址与IP地址静态绑定。比如，在/etc/dnsmasq.conf中针对MAC地址08:00:27:81:51:85，分配机器名vbox-xp，分配IP地址192.168.56.2</p>

<pre><code class="plain">dhcp-host=vbox-xp,08:00:27:81:51:85,192.168.56.2
</code></pre>

<p>最后，不要忘了重启动dnsmasq。</p>
]]></content>
  </entry>
  
</feed>
