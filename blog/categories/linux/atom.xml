<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Like的世界]]></title>
  <link href="http://www.malike.net.cn/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://www.malike.net.cn/"/>
  <updated>2017-02-05T13:48:17+00:00</updated>
  <id>http://www.malike.net.cn/</id>
  <author>
    <name><![CDATA[Like Ma]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Shadowsocks简介]]></title>
    <link href="http://www.malike.net.cn/blog/2016/11/29/shadowsocks-tutorial/"/>
    <updated>2016-11-29T01:39:55+00:00</updated>
    <id>http://www.malike.net.cn/blog/2016/11/29/shadowsocks-tutorial</id>
    <content type="html"><![CDATA[<p><a href="https://shadowsocks.org/en/index.html">Shadowsocks</a>（中文名: 影梭） 是一款开源的安全SOCKS 5代理，它主要用于在大陆翻墙。</p>

<h2>原理</h2>

<p>与SSH动态代理相似，客户端呈现为SOCKS 5代理服务，客户端与服务器之间采用加密通信。服务器部署于GFW之外，从而实现代理翻墙服务。</p>

<h2>特点</h2>

<ul>
<li>使用自行设计的协议加密通信，支持多种加密算法：AES、Blowfish、IDEA、RC4等。除创建TCP连接外无需握手，每次请求只转发1个连接，因此使用起来网速较快，在移动设备上较省电。</li>
<li>通过异步I/O和事件驱动实现，响应速度快。</li>
<li>客户端支持主流操作系统平台：Windows、Linux、OS X、Android、iOS和OpenWrt</li>
</ul>


<h2>shadowsocks-libev简介</h2>

<p>网络中普遍采用Python版本的<a href="https://pypi.python.org/pypi/shadowsocks">shadowsocks</a>，该版本看似安装简单，却存在如下缺点：</p>

<ul>
<li>没有Linux操作系统原生安装包（如：RPM和DEB）。</li>
<li>没有操作系统的服务脚本（如init.d和upstart）。</li>
<li>Python程序占用内存较多，运行效率不佳。</li>
</ul>


<p>而<a href="https://github.com/shadowsocks/shadowsocks-libev">shadowsocks-libev</a>是Shadowsocks在嵌入式和低端设备的轻量级实现：</p>

<ul>
<li>纯C实现，不仅占用内存极小，且运行效率更快。</li>
<li>在几乎所有Linux平台都存在原生安装包，且具有对应平台的服务启动脚本。</li>
</ul>


<p>下面基于Ubuntu 14.04/16.04介绍它的安装和配置。</p>

<h2>安装shadowsocks-libev</h2>

<p>虽然shadowsocks-libev可以编译出DEB，但是默认没有提供Ubuntu的包仓库。为了方便安装，我将它构建于我的PPA中。</p>

<p>客户端和服务端的安装方法相同：</p>

<pre><code class="bash">sudo add-apt-repository ppa:likemartinma/net
sudo apt-get update
sudo apt-get install shadowsocks-libev
</code></pre>

<h2>配置shadowsocks-libev服务端</h2>

<p>编辑/etc/shadowsocks-libev/config.json：</p>

<pre><code class="json">{
    "server_port": 8388,
    "password": "&lt;共享密码&gt;",
    "timeout": 60,
    "method": "aes-256-cfb"
}
</code></pre>

<ul>
<li><code>server_port</code>为服务端监听端口</li>
<li><code>password</code>为客户端和服务端预设的共享密码，它最好由安全密码生成器生成（如<a href="https://www.lastpass.com/">LastPass</a>或<a href="http://keepass.info/">KeePass</a>），且长度不小于6个字符。</li>
<li><code>timeout</code>为连接超时时间。</li>
<li><code>method</code>为加密算法，<code>aes-256-cfb</code>的安全性较好。</li>
</ul>


<p>配置完成后，需重启：</p>

<pre><code class="bash">sudo service shadowsocks-libev restart
</code></pre>

<h2>配置shadowsocks-libev客户端</h2>

<p>创建/etc/shadowsocks-libev/client.json （文件名可修改）：</p>

<pre><code class="json">{
    "server": "&lt;服务端地址&gt;",
    "server_port": "&lt;服务端端口&gt;",
    "local_port": "22357",
    "password": "&lt;共享密码&gt;",
    "method": "aes-256-cfb"
}
</code></pre>

<h3>Ubuntu 14.04</h3>

<p>安装包没有提供系统服务脚本，故须自己创建Upstart脚本/etc/init/ss-local.conf （文件名可修改）：</p>

<pre><code># ss-local

description "shadowsocks client"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid nobody
setgid nogroup

exec ss-local -c /etc/shadowsocks-libev/client.json
</code></pre>

<p>为了client.json的安全：</p>

<pre><code class="bash">sudo chown -R root:nogroup /etc/shadowsocks-libev
sudo chmod 0750 /etc/shadowsocks-libev
sudo chmod 0640 /etc/shadowsocks-libev/client.json
</code></pre>

<p>启动客户端</p>

<pre><code class="bash">sudo start ss-local
</code></pre>

<p>最后，通过修改/etc/default/shadowsocks-libev的<code>START=no</code>禁止在客户机启动服务端程序——它在客户机没有作用。</p>

<pre><code class="bash">sudo service shadowsocks-libev stop
</code></pre>

<h3>Ubuntu 16.04</h3>

<p>安装包提供了systemd的服务模板/lib/systemd/system/shadowsocks-libev-local@.service:</p>

<pre><code class="bash">sudo systemctl enable shadowsocks-libev-local@client
sudo systemctl start shadowsocks-libev-local@client
</code></pre>

<p>注意，@client与client.json的基本名必须一致。</p>

<p>最后，禁用并停止服务端程序：</p>

<pre><code class="bash">sudo systemctl disable shadowsocks-libev
sudo systemctl stop shadowsocks-libev
</code></pre>

<h2>集群化</h2>

<p>类似SSH集群，多个shadowsocks也可以构建SOCKS 5集群，具体请参考《<a href="http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster/">SSH翻墙集群</a>》的“HAProxy的配置方法”。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH翻墙集群]]></title>
    <link href="http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster/"/>
    <updated>2015-03-15T13:30:43+00:00</updated>
    <id>http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster</id>
    <content type="html"><![CDATA[<p>SSH动态代理是国内较为常见的翻墙方法。正如<a href="/blog/2014/10/27/ssh-tunnel-tutorial/">SSH隧道简介</a>所说，它存在不少有优点。</p>

<p>然而在实际使用中，它存在如下缺点：</p>

<ul>
<li>与PPTP等VPN协议相比，它的连接不稳定。前者应该具备协议级断线重传机制。</li>
<li>基于廉价VPS，导致它的连接不稳定。而且廉价VPS容易掉线，有时需要用户自己找在线客户修复，进一步延长了掉线时间。</li>
<li>由于上述缺点，不适合小型公司多人使用。</li>
</ul>


<p>在大概2年前，我摸索出SSH动态代理集群的办法。并将之部署于我所服务的公司，成功负载了20-30人日常翻墙学习与工作的需求。</p>

<h2>原理</h2>

<p>SSH动态代理，即为SOCK5代理，所以我们需要的是SOCK5集群。</p>

<p>若搜索<a href="https://www.google.com/search?hl=en&amp;q=socks+5+load+balance">socks 5 load balance</a>不难发现一些有用信息：</p>

<p><a href="http://serverfault.com/questions/517971/what-is-the-best-way-to-load-balance-multiple-sock5-proxys-on-seperate-vms-in-t">What is the best way to load balance multiple sock5 proxys on seperate VM&rsquo;s in the same datacenter?</a></p>

<p>我将分别介绍3种方法搭建SOCK5集群：</p>

<ol>
<li>利用第三方模块<a href="https://github.com/yaoweibin/nginx_tcp_proxy_module">nginx_tcp_proxy_module</a>。</li>
<li>Nginx 1.9开始支持<a href="http://nginx.com/resources/admin-guide/tcp-load-balancing/">TCP Load Balancing</a>。</li>
<li><a href="http://www.haproxy.org/">HAProxy</a></li>
</ol>


<p>关于SSH动态代理的配置方法，请参看<a href="/blog/2014/12/23/autossh-tutorial/">AutoSSH简介</a></p>

<h2>nginx_tcp_proxy_module的配置方法</h2>

<p>Ubuntu的Nginx并没有将nginx_tcp_proxy_module编译进去。为了简化安装，我基于Ubuntu的Nginx包，做了Nginx的<a href="https://launchpad.net/~likemartinma/+archive/ubuntu/net">PPA</a>:</p>

<ul>
<li>升级Nginx版本</li>
<li>加入nginx_tcp_proxy_module</li>
</ul>


<p>添加我的PPA</p>

<pre><code class="bash">sudo add-apt-repository ppa:likemartinma/net
sudo apt-get -y update
</code></pre>

<p>若未安装nginx，则</p>

<pre><code class="bash">sudo apt-get install -y nginx
</code></pre>

<p>若已安装nginx，则</p>

<pre><code class="bash">sudo apt-get -y upgrade
</code></pre>

<p>在/etc/nginx/nginx.conf中，增加如下内容：</p>

<pre><code class="nginx">tcp {
    access_log /var/log/nginx/tcp_access.log;

    upstream ssh_cluster {
        # simple round-robin
        server 127.0.0.1:12345;
        server 127.0.0.1:12346;
        server 127.0.0.1:12347;

        check interval=3000 rise=2 fall=5 timeout=1000;
    }

    server {
        listen 9999;
        proxy_pass ssh_cluster;
    }
}
</code></pre>

<p>为了查看集群的状态，在/etc/nginx/sites-enabled/default的中，增加如下内容：</p>

<pre><code class="nginx">server {
    ...

    location /status {
        tcp_check_status;
    }
}
</code></pre>

<p>重启Nginx:</p>

<pre><code>service nginx restart
</code></pre>

<p>如此，访问http://&lt;cluster IP&gt;/status将能查看集群的详细状态。</p>

<h2>Nginx 1.9的配置方法</h2>

<p>Ubuntu 15.10之前的官方Nginx版本都小于1.9，须通过ppa:nginx/development升级nginx。</p>

<p>添加ppa:nginx/development</p>

<pre><code class="bash">sudo add-apt-repository ppa:nginx/development
sudo apt-get -y update
</code></pre>

<p>若未安装nginx，则</p>

<pre><code class="bash">sudo apt-get install -y nginx
</code></pre>

<p>若已安装nginx，则</p>

<pre><code class="bash">sudo apt-get -y upgrade
</code></pre>

<p>在/etc/nginx/nginx.conf中，增加如下内容：</p>

<pre><code class="nginx">stream {
    upstream ssh_cluster {
        least_conn;
        server 127.0.0.1:12345;
        server 127.0.0.1:12346;
        server 127.0.0.1:12347;
    }

    server {
        listen 9999;
        proxy_pass ssh_cluster;
    }
}
</code></pre>

<p>重启Nginx:</p>

<pre><code>service nginx restart
</code></pre>

<h2>HAProxy的配置方法</h2>

<p>安装haproxy</p>

<pre><code>sudo apt-get install -y haproxy
</code></pre>

<p>在/etc/haproxy/haproxy.cfg中，增加如下内容：</p>

<pre><code>frontend socks5
    mode tcp
    bind *:9999
    default_backend ssh_cluster

backend ssh_cluster
    mode tcp
    balance roundrobin
    server vps1 127.0.0.1:12345 weight 1 check inter 30000
    server vps2 127.0.0.1:12346 weight 1 check inter 30000
    server vps3 127.0.0.1:12347 weight 1 check inter 30000
</code></pre>

<p>为了查看集群的状态，在/etc/haproxy/haproxy.cfg中，增加如下内容：</p>

<pre><code>listen stats :9090
    balance
    mode http
    stats enable
    stats auth admin:admin
</code></pre>

<p>默认安装，haproxy处于不活动状态，须要激活它。</p>

<p>在/etc/default/haproxy中，修改如下行：</p>

<pre><code>ENABLED=1
</code></pre>

<p>最后，启动haproxy:</p>

<pre><code>service haproxy start
</code></pre>

<p>如此，访问http://&lt;cluster IP&gt;:9090/haproxy?stats将能查看集群的详细状态。</p>

<h2>总结</h2>

<ul>
<li>nginx_tcp_proxy_module有简单的集群状态页面。</li>
<li>nginx 1.9没有集群状态查页面，仅能通过错误日志/var/log/ngnix/error.log来查看掉线的集群节点。</li>
<li>haproxy不仅有完善的集群状态页面，而且不需要任何PPA，应该是最佳选择。</li>
<li>上述3种方法都缺乏认证机制，只能部署于家庭或企业内网。当然也可以部署于个人电脑，事实上，我就是这样使用的。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AutoSSH简介]]></title>
    <link href="http://www.malike.net.cn/blog/2014/12/23/autossh-tutorial/"/>
    <updated>2014-12-23T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/12/23/autossh-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://www.harding.motd.ca/autossh/">autossh</a> (Automatically restart SSH sessions and tunnels)，它在运行的时候启动一个SSH进程，并监控该进程的健康状况；当SSH进程崩溃或停止通信时，它将重启动SSH进程。</p>

<h2>命令选项</h2>

<pre><code class="bash">autossh [-V] [-M port[:echo_port]] [-f] [SSH_OPTIONS]
</code></pre>

<ul>
<li><p><strong>-M port[:echo_port]</strong> 指定监控端口（和echo端口，默认为前者加1）。</p>

<ul>
<li>若希望使远程标准inetd的echo服务（默认端口为7），则指定echo_port，仅需服务监听地址为localhost。</li>
<li>若port设置为0，则将禁用监控功能。仅在ssh退出后重启它。</li>
</ul>
</li>
<li><p><strong>-f</strong> 使autossh在后台运行。</p></li>
</ul>


<p>另外，autossh还提供了一组环境变量来控制其行为, 这里仅介绍几个有代表性的，其可以man autossh</p>

<ul>
<li><strong>AUTOSSH_FIRST_POLL</strong> 指定首次论询测试时间。</li>
<li><strong>AUTOSSH_POLL</strong> 指定连接论询时间，默认600。若该值小于两次网络超时（默认15秒），则网络超将被调整为该值的1/2</li>
<li><strong>AUTOSSH_GATETIME</strong> 指定等待SSH连接成功建立的时间，默认30秒，超时表示首次运行失败，将退出autossh。若设为0，则禁用该功能，通常用于启动时运行autossh。</li>
<li><strong>AUTOSSH_MAXLIFETIME</strong> 指autossh最长运行时间，达到该时间，autossh将退出，并杀死SSH进程。</li>
<li><strong>AUTOSSH_MAXSTART</strong> 指定SSH最大启动次数。默认-1，表示无限制。</li>
</ul>


<h2>Ubuntu配置方法</h2>

<p>基于Ubuntu 12.04或14.04，以SSH动态代理（即SSH翻墙）为例。init script和Upstart都可以将autossh变成服务，然Upstart的<em>respawn</em>容错能力更强，它能在服务进程掉线，重新启动该服务。</p>

<pre><code># autossh

description "autossh daemon"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid like
setgid like

exec /usr/bin/autossh -M64000 -q -N -D localhost:12348 sshproxy
</code></pre>

<ul>
<li><em>setuid</em>和<em>setgid</em>为了让autossh运行在指定的用户和用户组上。</li>
<li><em>start on</em>表示当eth0或wlan0激活时，启动autossh，<em>stop on</em>反之。其目的为避免系统启动或网络掉线时，频繁尝试启动autossh。</li>
</ul>


<h2>更好的办法</h2>

<p>最近OpenSSH都支持选项<strong>ServerAliveInterval</strong>和<strong>ServerAliveCountMax</strong>，实际为建立在SSH协议上的心跳测试。当测试失败后，SSH客户端进程将退出。通过Upstart的respawn功能重启SSH客户端进程，也能达到autossh目的。</p>

<p>仍以SSH动态代理为例：</p>

<pre><code># sshproxy

description "ssh proxy"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid like
setgid like

exec /usr/bin/ssh \
    -oServerAliveInterval=300 \
    -oServerAliveCountMax=2 \
    -q -N -D localhost:12348 sshproxy
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH隧道简介]]></title>
    <link href="http://www.malike.net.cn/blog/2014/10/27/ssh-tunnel-tutorial/"/>
    <updated>2014-10-27T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/10/27/ssh-tunnel-tutorial</id>
    <content type="html"><![CDATA[<p>SSH的功能非常强大，日常除了用于命令行远程登录服务器。它还具有神奇的隧道（tunnel）功能（也被称为SSH代理），可用于加密访问本地或远程主机的服务。</p>

<p>通常，SSH代理具有3种方式：</p>

<h2>SSH（正向）代理</h2>

<p>通过参数-L [bind_address:]port:remote_host:remote_port，将指定本地（客户端）端口转发至远程端口上。</p>

<p><img src="/images/ssh-tunnel-proxy.png" alt="SSH Proxy" /></p>

<p>如上图， hosta无法直接访问hostb，但它能直接SSH登录gateway；如此通过gateway，将hosta的端口X转发至hostb的端口Y上。相当于端口X和端口Y之间建立了加密隧道。</p>

<p>一般来说，端口Y为hostb上某服务的监听端口。当建立隧道后，hosta将监听端口X。应用程序访问hosta的端口X，等同于访问hostb的端口Y。对于应用程序，hostb端口Y对应的服务就如同运行在hosta上。</p>

<p>日常工作中，客户的网络常由于信息安全而被网关（或防火墙）隔离。当我们的软件在客户网络中某服务器发生问题时，我们常需奔赴客户现场进行调试。若客户存在某机器安装了SSH服务器，且能被外部访问。就可以利用SSH正向代理的方法，快速简便的登录被隔离的服务器并进行应用调试。</p>

<h2>SSH反向代理</h2>

<p>通过参数-R [bind_address:]port:remote_host:remote_port，将指定远程端口转发至本地（客户端）端口上。</p>

<p><img src="/images/ssh-tunnel-reverse-proxy.png" alt="SSH Reverse Proxy" /></p>

<p>如上图，hosta在防火墙内，无法被hostb直接访问。但它能直接SSH登录hostb；如此通过hostb，将hostb的端口X转发至hosta的端口Y上。该方法与SSH正向代理类似，所不同的是该隧道的访问方向是从服务端（hostb）至客户端(hosta），故被称为反向代理。</p>

<p>其应用场景也与SSH正向代理类似，所不同的是若客户不存在可供外部访问的SSH服务器时，我们可以在外网建设一个SSH服务器给客户的被隔离服务器来建立隧道。如此，我们可以访问自己的SSH服务器对应端口来调试客户服务器的应用。</p>

<p>更进一步，客户内网甚至不能访问外网，此时可利用客户内网一台笔记本（或台式机，它可以访问目标服务器）USB接上3G/4G手机来达到访问外部SSH服务器的目的。</p>

<p><img src="/images/ssh-tunnel-reverse-proxy-mobile.png" alt="SSH Reverse Proxy Mobile" /></p>

<h2>SSH动态代理</h2>

<p>通过参数-D [bind_address:]port，利用远程服务器为访问出口，在本地建立SOCKS 4/5代理服务器。</p>

<p>可以形象描绘为将本地应用的端口（SOCKS客户端端口），动态转发至远程。</p>

<p><img src="/images/ssh-tunnel-dynamic-proxy.png" alt="SSH Dynamic Proxy" /></p>

<p>该功能广为人知的应用场景为翻墙。如上图，在国外租用VPS（hostb），客户端（hosta）通过SSH动态代理端口X（SOCKS 4/5的端口）便可以访问被GFW封锁的网络。</p>

<p>这种翻墙最大的优势在于</p>

<ul>
<li>低成本：国外廉价低配置VPS基本满足个人翻墙需求。</li>
<li>服务端0配置：服务端只需要安装SSH服务端。</li>
<li>客户端配置简单：客户端需要安装SSH客户端，以及一条命令。</li>
<li>加密隧道：保证网络访问的数据安全。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[创建通过SSH访问的chroot]]></title>
    <link href="http://www.malike.net.cn/blog/2014/08/07/create-chroot-for-ssh-access/"/>
    <updated>2014-08-07T10:09:48+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/08/07/create-chroot-for-ssh-access</id>
    <content type="html"><![CDATA[<p>我的工作环境是Ubuntu，然而经常需要CentOS来编译或测试。一般存在3种解决办法：</p>

<ul>
<li>创建VirtualBox或KVM虚拟机：

<ul>
<li>优点：部署容易，且可以运行各种应用（如Oracle）。</li>
<li>缺点：运行速度相对LXC和chroot慢，特别是I/O.</li>
</ul>
</li>
<li>创建CentOS的LXC：

<ul>
<li>优点：运行速度快，且具有独立IP，可以通过SSH访问。</li>
<li>缺点: 需要修改启动脚本。</li>
</ul>
</li>
<li>创建CentOS的chroot：

<ul>
<li>优点：不需要修改任何配置。</li>
<li>缺点：无法直接通过SSH访问，且需要root权限才能运行chroot。</li>
</ul>
</li>
</ul>


<h2>创建CentOS的chroot</h2>

<p>具体参看<a href="/blog/2014/07/15/rinse-tutorial/">Rinse简介</a></p>

<h2>创建SSH用户（或组）</h2>

<p>在当前OS环境中，创建用户centos6.</p>

<pre><code class="sh">sudo useradd -m -s /bin/bash centos6
</code></pre>

<p>获取centos6的uid和gid</p>

<pre><code class="sh">id -u centos6
id -g centos6
</code></pre>

<p>在chroot环境中创建同名用户，且保持uid和gid相同。</p>

<pre><code class="sh">sudo chroot /var/lib/centos-6 /bin/bash
groupadd -g &lt;gid&gt; centos6
useradd -s /bin/bash -m -u &lt;uid&gt; -g centos6 centos6
</code></pre>

<h2>配置sshd</h2>

<p>将下述内容追加至/etc/ssh/sshd_config</p>

<pre><code>Match group centos6
    ChrootDirectory /var/lib/centos-6
</code></pre>

<p>确保/var/lib/centos-6的每一级目录的属主为root，且其他用户或组没有写权限。</p>

<p>然后，重启ssh</p>

<pre><code>service ssh restart
</code></pre>

<p>这样保证centos6组的用户登录ssh时，chroot至/var/lib/centos-6目录中。</p>

<h2>配置dev, proc和sysfs</h2>

<p>chroot环境中rpm安装和卸载的前/后置脚本依赖dev, proc和sysfs，否则可能将造成安装和卸载错误。</p>

<p>在/etc/fstab中，增加proc和sysfs的挂载选项</p>

<pre><code>proc /var/lib/centos-6/proc proc  defaults      0 0
none /var/lib/centos-6/sys  sysfs defaults      0 0
/dev /var/lib/centos-6/dev  none  defaults,bind 0 0
</code></pre>

<h2>配置ssh密钥登录</h2>

<p>chroot的ssh密钥登录与常规情况的唯一区别在于公钥应存放在当前OS环境（而非chroot环境）的~/.ssh/authorized_keys，因为sshd在执行chroot之前需要检查公钥是否正确。</p>

<p>因此，本例中应该存放在当前OS的/home/centos-6/.ssh/authorized_keys.</p>
]]></content>
  </entry>
  
</feed>
