<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ssh | Like的世界]]></title>
  <link href="http://www.malike.net.cn/blog/categories/ssh/atom.xml" rel="self"/>
  <link href="http://www.malike.net.cn/"/>
  <updated>2019-08-19T05:55:27+00:00</updated>
  <id>http://www.malike.net.cn/</id>
  <author>
    <name><![CDATA[Like Ma]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SSH翻墙集群]]></title>
    <link href="http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster/"/>
    <updated>2015-03-15T21:30:43+08:00</updated>
    <id>http://www.malike.net.cn/blog/2015/03/15/ssh-proxy-cluster</id>
    <content type="html"><![CDATA[<p>SSH动态代理是国内较为常见的翻墙方法。正如<a href="/blog/2014/10/27/ssh-tunnel-tutorial/">SSH隧道简介</a>所说，它存在不少有优点。</p>

<p>然而在实际使用中，它存在如下缺点：</p>

<ul>
<li>与PPTP等VPN协议相比，它的连接不稳定。前者应该具备协议级断线重传机制。</li>
<li>基于廉价VPS，导致它的连接不稳定。而且廉价VPS容易掉线，有时需要用户自己找在线客户修复，进一步延长了掉线时间。</li>
<li>由于上述缺点，不适合小型公司多人使用。</li>
</ul>


<p>在大概2年前，我摸索出SSH动态代理集群的办法。并将之部署于我所服务的公司，成功负载了20-30人日常翻墙学习与工作的需求。</p>

<h2>原理</h2>

<p>SSH动态代理，即为SOCK5代理，所以我们需要的是SOCK5集群。</p>

<p>若搜索<a href="https://www.google.com/search?hl=en&amp;q=socks+5+load+balance">socks 5 load balance</a>不难发现一些有用信息：</p>

<p><a href="http://serverfault.com/questions/517971/what-is-the-best-way-to-load-balance-multiple-sock5-proxys-on-seperate-vms-in-t">What is the best way to load balance multiple sock5 proxys on seperate VM&rsquo;s in the same datacenter?</a></p>

<p>我将分别介绍3种方法搭建SOCK5集群：</p>

<ol>
<li>利用第三方模块<a href="https://github.com/yaoweibin/nginx_tcp_proxy_module">nginx_tcp_proxy_module</a>。</li>
<li>Nginx 1.9开始支持<a href="http://nginx.com/resources/admin-guide/tcp-load-balancing/">TCP Load Balancing</a>。</li>
<li><a href="http://www.haproxy.org/">HAProxy</a></li>
</ol>


<p>关于SSH动态代理的配置方法，请参看<a href="/blog/2014/12/23/autossh-tutorial/">AutoSSH简介</a></p>

<h2>nginx_tcp_proxy_module的配置方法</h2>

<p>Ubuntu的Nginx并没有将nginx_tcp_proxy_module编译进去。为了简化安装，我基于Ubuntu的Nginx包，做了Nginx的<a href="https://launchpad.net/~likemartinma/+archive/ubuntu/net">PPA</a>:</p>

<ul>
<li>升级Nginx版本</li>
<li>加入nginx_tcp_proxy_module</li>
</ul>


<p>添加我的PPA</p>

<pre><code class="bash">sudo add-apt-repository ppa:likemartinma/net
sudo apt-get -y update
</code></pre>

<p>若未安装nginx，则</p>

<pre><code class="bash">sudo apt-get install -y nginx
</code></pre>

<p>若已安装nginx，则</p>

<pre><code class="bash">sudo apt-get -y upgrade
</code></pre>

<p>在/etc/nginx/nginx.conf中，增加如下内容：</p>

<pre><code class="nginx">tcp {
    access_log /var/log/nginx/tcp_access.log;

    upstream ssh_cluster {
        # simple round-robin
        server 127.0.0.1:12345;
        server 127.0.0.1:12346;
        server 127.0.0.1:12347;

        check interval=3000 rise=2 fall=5 timeout=1000;
    }

    server {
        listen 9999;
        proxy_pass ssh_cluster;
    }
}
</code></pre>

<p>为了查看集群的状态，在/etc/nginx/sites-enabled/default的中，增加如下内容：</p>

<pre><code class="nginx">server {
    ...

    location /status {
        tcp_check_status;
    }
}
</code></pre>

<p>重启Nginx:</p>

<pre><code>service nginx restart
</code></pre>

<p>如此，访问http://&lt;cluster IP&gt;/status将能查看集群的详细状态。</p>

<h2>Nginx 1.9的配置方法</h2>

<p>Ubuntu 15.10之前的官方Nginx版本都小于1.9，须通过ppa:nginx/development升级nginx。</p>

<p>添加ppa:nginx/development</p>

<pre><code class="bash">sudo add-apt-repository ppa:nginx/development
sudo apt-get -y update
</code></pre>

<p>若未安装nginx，则</p>

<pre><code class="bash">sudo apt-get install -y nginx
</code></pre>

<p>若已安装nginx，则</p>

<pre><code class="bash">sudo apt-get -y upgrade
</code></pre>

<p>在/etc/nginx/nginx.conf中，增加如下内容：</p>

<pre><code class="nginx">stream {
    upstream ssh_cluster {
        least_conn;
        server 127.0.0.1:12345;
        server 127.0.0.1:12346;
        server 127.0.0.1:12347;
    }

    server {
        listen 9999;
        proxy_pass ssh_cluster;
    }
}
</code></pre>

<p>重启Nginx:</p>

<pre><code>service nginx restart
</code></pre>

<h2>HAProxy的配置方法</h2>

<p>安装haproxy</p>

<pre><code>sudo apt-get install -y haproxy
</code></pre>

<p>在/etc/haproxy/haproxy.cfg中，增加如下内容：</p>

<pre><code>frontend socks5
    mode tcp
    bind *:9999
    default_backend ssh_cluster

backend ssh_cluster
    mode tcp
    balance roundrobin
    server vps1 127.0.0.1:12345 weight 1 check inter 30000
    server vps2 127.0.0.1:12346 weight 1 check inter 30000
    server vps3 127.0.0.1:12347 weight 1 check inter 30000
</code></pre>

<p>为了查看集群的状态，在/etc/haproxy/haproxy.cfg中，增加如下内容：</p>

<pre><code>listen stats :9090
    balance
    mode http
    stats enable
    stats auth admin:admin
</code></pre>

<p>默认安装，haproxy处于不活动状态，须要激活它。</p>

<p>在/etc/default/haproxy中，修改如下行：</p>

<pre><code>ENABLED=1
</code></pre>

<p>最后，启动haproxy:</p>

<pre><code>service haproxy start
</code></pre>

<p>如此，访问http://&lt;cluster IP&gt;:9090/haproxy?stats将能查看集群的详细状态。</p>

<h2>总结</h2>

<ul>
<li>nginx_tcp_proxy_module有简单的集群状态页面。</li>
<li>nginx 1.9没有集群状态查页面，仅能通过错误日志/var/log/ngnix/error.log来查看掉线的集群节点。</li>
<li>haproxy不仅有完善的集群状态页面，而且不需要任何PPA，应该是最佳选择。</li>
<li>上述3种方法都缺乏认证机制，只能部署于家庭或企业内网。当然也可以部署于个人电脑，事实上，我就是这样使用的。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AutoSSH简介]]></title>
    <link href="http://www.malike.net.cn/blog/2014/12/23/autossh-tutorial/"/>
    <updated>2014-12-23T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/12/23/autossh-tutorial</id>
    <content type="html"><![CDATA[<p><a href="http://www.harding.motd.ca/autossh/">autossh</a> (Automatically restart SSH sessions and tunnels)，它在运行的时候启动一个SSH进程，并监控该进程的健康状况；当SSH进程崩溃或停止通信时，它将重启动SSH进程。</p>

<h2>命令选项</h2>

<pre><code class="bash">autossh [-V] [-M port[:echo_port]] [-f] [SSH_OPTIONS]
</code></pre>

<ul>
<li><p><strong>-M port[:echo_port]</strong> 指定监控端口（和echo端口，默认为前者加1）。</p>

<ul>
<li>若希望使远程标准inetd的echo服务（默认端口为7），则指定echo_port，仅需服务监听地址为localhost。</li>
<li>若port设置为0，则将禁用监控功能。仅在ssh退出后重启它。</li>
</ul>
</li>
<li><p><strong>-f</strong> 使autossh在后台运行。</p></li>
</ul>


<p>另外，autossh还提供了一组环境变量来控制其行为, 这里仅介绍几个有代表性的，其可以man autossh</p>

<ul>
<li><strong>AUTOSSH_FIRST_POLL</strong> 指定首次论询测试时间。</li>
<li><strong>AUTOSSH_POLL</strong> 指定连接论询时间，默认600。若该值小于两次网络超时（默认15秒），则网络超将被调整为该值的1/2</li>
<li><strong>AUTOSSH_GATETIME</strong> 指定等待SSH连接成功建立的时间，默认30秒，超时表示首次运行失败，将退出autossh。若设为0，则禁用该功能，通常用于启动时运行autossh。</li>
<li><strong>AUTOSSH_MAXLIFETIME</strong> 指autossh最长运行时间，达到该时间，autossh将退出，并杀死SSH进程。</li>
<li><strong>AUTOSSH_MAXSTART</strong> 指定SSH最大启动次数。默认-1，表示无限制。</li>
</ul>


<h2>Ubuntu配置方法</h2>

<p>基于Ubuntu 12.04或14.04，以SSH动态代理（即SSH翻墙）为例。init script和Upstart都可以将autossh变成服务，然Upstart的<em>respawn</em>容错能力更强，它能在服务进程掉线，重新启动该服务。</p>

<pre><code># autossh

description "autossh daemon"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid like
setgid like

exec /usr/bin/autossh -M64000 -q -N -D localhost:12348 sshproxy
</code></pre>

<ul>
<li><em>setuid</em>和<em>setgid</em>为了让autossh运行在指定的用户和用户组上。</li>
<li><em>start on</em>表示当eth0或wlan0激活时，启动autossh，<em>stop on</em>反之。其目的为避免系统启动或网络掉线时，频繁尝试启动autossh。</li>
</ul>


<h2>更好的办法</h2>

<p>最近OpenSSH都支持选项<strong>ServerAliveInterval</strong>和<strong>ServerAliveCountMax</strong>，实际为建立在SSH协议上的心跳测试。当测试失败后，SSH客户端进程将退出。通过Upstart的respawn功能重启SSH客户端进程，也能达到autossh目的。</p>

<p>仍以SSH动态代理为例：</p>

<pre><code># sshproxy

description "ssh proxy"

start on (net-device-up IFACE=eth0 or net-device-up IFACE=wlan0)
stop on (net-device-down IFACE=eth0 and net-device-down IFACE=wlan0)

respawn

setuid like
setgid like

exec /usr/bin/ssh \
    -oServerAliveInterval=300 \
    -oServerAliveCountMax=2 \
    -q -N -D localhost:12348 sshproxy
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH隧道简介]]></title>
    <link href="http://www.malike.net.cn/blog/2014/10/27/ssh-tunnel-tutorial/"/>
    <updated>2014-10-27T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/10/27/ssh-tunnel-tutorial</id>
    <content type="html"><![CDATA[<p>SSH的功能非常强大，日常除了用于命令行远程登录服务器。它还具有神奇的隧道（tunnel）功能（也被称为SSH代理），可用于加密访问本地或远程主机的服务。</p>

<p>通常，SSH代理具有3种方式：</p>

<h2>SSH（正向）代理</h2>

<p>通过参数-L [bind_address:]port:remote_host:remote_port，将指定本地（客户端）端口转发至远程端口上。</p>

<p><img src="/images/ssh-tunnel-proxy.png" alt="SSH Proxy" /></p>

<p>如上图， hosta无法直接访问hostb，但它能直接SSH登录gateway；如此通过gateway，将hosta的端口X转发至hostb的端口Y上。相当于端口X和端口Y之间建立了加密隧道。</p>

<p>一般来说，端口Y为hostb上某服务的监听端口。当建立隧道后，hosta将监听端口X。应用程序访问hosta的端口X，等同于访问hostb的端口Y。对于应用程序，hostb端口Y对应的服务就如同运行在hosta上。</p>

<p>日常工作中，客户的网络常由于信息安全而被网关（或防火墙）隔离。当我们的软件在客户网络中某服务器发生问题时，我们常需奔赴客户现场进行调试。若客户存在某机器安装了SSH服务器，且能被外部访问。就可以利用SSH正向代理的方法，快速简便的登录被隔离的服务器并进行应用调试。</p>

<h2>SSH反向代理</h2>

<p>通过参数-R [bind_address:]port:remote_host:remote_port，将指定远程端口转发至本地（客户端）端口上。</p>

<p><img src="/images/ssh-tunnel-reverse-proxy.png" alt="SSH Reverse Proxy" /></p>

<p>如上图，hosta在防火墙内，无法被hostb直接访问。但它能直接SSH登录hostb；如此通过hostb，将hostb的端口X转发至hosta的端口Y上。该方法与SSH正向代理类似，所不同的是该隧道的访问方向是从服务端（hostb）至客户端(hosta），故被称为反向代理。</p>

<p>其应用场景也与SSH正向代理类似，所不同的是若客户不存在可供外部访问的SSH服务器时，我们可以在外网建设一个SSH服务器给客户的被隔离服务器来建立隧道。如此，我们可以访问自己的SSH服务器对应端口来调试客户服务器的应用。</p>

<p>更进一步，客户内网甚至不能访问外网，此时可利用客户内网一台笔记本（或台式机，它可以访问目标服务器）USB接上3G/4G手机来达到访问外部SSH服务器的目的。</p>

<p><img src="/images/ssh-tunnel-reverse-proxy-mobile.png" alt="SSH Reverse Proxy Mobile" /></p>

<h2>SSH动态代理</h2>

<p>通过参数-D [bind_address:]port，利用远程服务器为访问出口，在本地建立SOCKS 4/5代理服务器。</p>

<p>可以形象描绘为将本地应用的端口（SOCKS客户端端口），动态转发至远程。</p>

<p><img src="/images/ssh-tunnel-dynamic-proxy.png" alt="SSH Dynamic Proxy" /></p>

<p>该功能广为人知的应用场景为翻墙。如上图，在国外租用VPS（hostb），客户端（hosta）通过SSH动态代理端口X（SOCKS 4/5的端口）便可以访问被GFW封锁的网络。</p>

<p>这种翻墙最大的优势在于</p>

<ul>
<li>低成本：国外廉价低配置VPS基本满足个人翻墙需求。</li>
<li>服务端0配置：服务端只需要安装SSH服务端。</li>
<li>客户端配置简单：客户端需要安装SSH客户端，以及一条命令。</li>
<li>加密隧道：保证网络访问的数据安全。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[创建通过SSH访问的chroot]]></title>
    <link href="http://www.malike.net.cn/blog/2014/08/07/create-chroot-for-ssh-access/"/>
    <updated>2014-08-07T18:09:48+08:00</updated>
    <id>http://www.malike.net.cn/blog/2014/08/07/create-chroot-for-ssh-access</id>
    <content type="html"><![CDATA[<p>我的工作环境是Ubuntu，然而经常需要CentOS来编译或测试。一般存在3种解决办法：</p>

<ul>
<li>创建VirtualBox或KVM虚拟机：

<ul>
<li>优点：部署容易，且可以运行各种应用（如Oracle）。</li>
<li>缺点：运行速度相对LXC和chroot慢，特别是I/O.</li>
</ul>
</li>
<li>创建CentOS的LXC：

<ul>
<li>优点：运行速度快，且具有独立IP，可以通过SSH访问。</li>
<li>缺点: 需要修改启动脚本。</li>
</ul>
</li>
<li>创建CentOS的chroot：

<ul>
<li>优点：不需要修改任何配置。</li>
<li>缺点：无法直接通过SSH访问，且需要root权限才能运行chroot。</li>
</ul>
</li>
</ul>


<h2>创建CentOS的chroot</h2>

<p>具体参看<a href="/blog/2014/07/15/rinse-tutorial/">Rinse简介</a></p>

<h2>创建SSH用户（或组）</h2>

<p>在当前OS环境中，创建用户centos6.</p>

<pre><code class="sh">sudo useradd -m -s /bin/bash centos6
</code></pre>

<p>获取centos6的uid和gid</p>

<pre><code class="sh">id -u centos6
id -g centos6
</code></pre>

<p>在chroot环境中创建同名用户，且保持uid和gid相同。</p>

<pre><code class="sh">sudo chroot /var/lib/centos-6 /bin/bash
groupadd -g &lt;gid&gt; centos6
useradd -s /bin/bash -m -u &lt;uid&gt; -g centos6 centos6
</code></pre>

<h2>配置sshd</h2>

<p>将下述内容追加至/etc/ssh/sshd_config</p>

<pre><code>Match group centos6
    ChrootDirectory /var/lib/centos-6
</code></pre>

<p>确保/var/lib/centos-6的每一级目录的属主为root，且其他用户或组没有写权限。</p>

<p>然后，重启ssh</p>

<pre><code>service ssh restart
</code></pre>

<p>这样保证centos6组的用户登录ssh时，chroot至/var/lib/centos-6目录中。</p>

<h2>配置dev, proc和sysfs</h2>

<p>chroot环境中rpm安装和卸载的前/后置脚本依赖dev, proc和sysfs，否则可能将造成安装和卸载错误。</p>

<p>在/etc/fstab中，增加proc和sysfs的挂载选项</p>

<pre><code>proc /var/lib/centos-6/proc proc  defaults      0 0
none /var/lib/centos-6/sys  sysfs defaults      0 0
/dev /var/lib/centos-6/dev  none  defaults,bind 0 0
</code></pre>

<h2>配置ssh密钥登录</h2>

<p>chroot的ssh密钥登录与常规情况的唯一区别在于公钥应存放在当前OS环境（而非chroot环境）的~/.ssh/authorized_keys，因为sshd在执行chroot之前需要检查公钥是否正确。</p>

<p>因此，本例中应该存放在当前OS的/home/centos-6/.ssh/authorized_keys.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用SSH限制rsync的访问目录]]></title>
    <link href="http://www.malike.net.cn/blog/2014/04/01/restrict-ssh-access-to-rsync/"/>
    <updated>2014-04-01T00:00:00+00:00</updated>
    <id>http://www.malike.net.cn/blog/2014/04/01/restrict-ssh-access-to-rsync</id>
    <content type="html"><![CDATA[<p>我经常使用VPS来共享数据给朋友同事。由于VPS通常内存较小，不太适合安装ftp服务器；而http服务器常常只有下载的功能；更重要是某些数据较大，每天可能都在改变，需要增量同步上传或下载。</p>

<p>rsync是这类场景的良好选择。一方面，通过SSH服务器来工作，它不常驻内存，节约了VPS内存使用；另一方面，它优秀的二进制增量同步功能，不仅减少了同步时间，也节约了VPS有限的带宽。</p>

<p>在实际使用过程中，我遇到了下述问题：</p>

<ul>
<li>针对某些目录，某些用户仅能读（下载）;</li>
<li>针对某些目录，某些用户仅能写（上传）;</li>
<li>具有读或写权限的用户不能登录SSH，也不能执行任何程序;</li>
<li>具有读或写权限的用户不能访问其他目录。</li>
</ul>


<p>诚然，这些问题可以通过操作系统用户相对复杂的权限控制（包括目录基本权限和ACL以及SELinux等）。然而，通过搜索和学习，我发现强大的SSH早已具备这样的接口：</p>

<ul>
<li>通过密钥对，简化用户使用，让用户彻底摆脱密码的记忆（当然用户加密私钥，还是要自己记密码的）；</li>
<li>通过~/.ssh/authorized_keys的<code>command</code>选项（通过<a href="http://man.he.net/man5/authorized_keys">man authorized_keys</a>阅读详细内容），设置脚本过滤掉业务上无效的指令；</li>
<li>还是通过command选项，限制不同密钥有不同权限，如密钥A只能读，密钥B只能写。</li>
</ul>


<p>下面，我将通过具体示例来解释command选项:</p>

<h2>限制用户仅能读</h2>

<pre><code class="sh">#!/bin/sh

DATA_DIR=/home/share/data

case "$SSH_ORIGINAL_COMMAND" in
rsync\ --server*)
        TARGET=`echo "$SSH_ORIGINAL_COMMAND" | awk '{ print $NF }'`
        if echo "$SSH_ORIGINAL_COMMAND" | grep -q " --sender " &amp;&amp; [ "$TARGET" = $DATA_DIR ]; then
                $SSH_ORIGINAL_COMMAND
        else
                echo "Rejected"
        fi
        ;;
*)
        echo "Rejected"
        ;;
esac
</code></pre>

<p>该脚本（存储在文件/usr/local/bin/validate_pull_share）限制用户share仅能下载<code>$DATA_DIR</code>的文件, 它需要跟公钥配置在服务器的/home/share/.ssh/authorized_keys，如：</p>

<pre><code>command="/usr/local/bin/validate_pull_share" ssh-rsa &lt;用户公钥A&gt;
</code></pre>

<ul>
<li><code>command</code>选项用于指定脚本，该脚本可以过滤一些远程命令（禁止执行），它相当于远程命令的前置脚本。</li>
<li><code>$SSH_ORIGINAL_COMMAND</code>是sshd传递给<code>command</code>脚本环境变量，表示ssh客户端需要远程执行的命令。</li>
<li>第6行限制仅能执行rsync服务端指令。</li>
<li>第7行获取<code>$SSH_ORIGINAL_COMMAND</code>的最后一个参数（这里没有考虑有空格的目录），这个参数在这里是rsync需要访问的目录$TARGET.</li>
<li>第8行为命令合法性判断, 由两个条件组成：

<ul>
<li>判断<code>--sender</code>参数是否存在于<code>$SSH_ORIGINAL_COMMAND</code>中，<code>--sender</code>表示该命令为下载命令；</li>
<li>判断下载目录$TARGET是否与规定的目录一致。</li>
</ul>
</li>
</ul>


<h2>限制用户仅能写</h2>

<pre><code class="sh">#!/bin/sh

DATA_DIR=/home/share/data

case "$SSH_ORIGINAL_COMMAND" in
rsync\ --server*)
        TARGET=`echo "$SSH_ORIGINAL_COMMAND" | awk '{ print $NF }'`
        if ! echo "$SSH_ORIGINAL_COMMAND" | grep -q " --sender " &amp;&amp; [ "$TARGET" = $DATA_DIR ]; then
                $SSH_ORIGINAL_COMMAND
        else
                echo "Rejected"
        fi
        ;;
*)
        echo "Rejected"
        ;;
esac
</code></pre>

<p>实际仅与读限制脚本相差一行（第8行），仅允许没有<code>--sender</code>参数的命令（即上传命令）。</p>

<p>该脚本（存储在文件/usr/local/bin/validate_push_share）同样需要跟另一个公钥配置在服务器的/home/share/.ssh/authorized_keys，如：</p>

<pre><code>command="/usr/local/bin/validate_push_share" ssh-rsa &lt;用户公钥B&gt;
</code></pre>

<h2>总结</h2>

<p>通过上述脚本和配置实现了不同密钥访问同一目录的不同权限控制。还可以在这些脚本基础上扩展到多个目录或文件的访问控制。以及访问时间的控制，如某时段只能读，另外时段只能写；避免正在写数据的时候，有用户读数据造成数据不一致的问题。</p>
]]></content>
  </entry>
  
</feed>
